{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import  stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csc_matrix, hstack\n",
    "from sklearn.model_selection import cross_validate, train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import defaultdict\n",
    "from matplotlib_venn import venn2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector():\n",
    "    def __init__(self, estimator,\n",
    "                       metric,\\\n",
    "                       larger_is_better,\\\n",
    "                       cv,\n",
    "                       use_values,\\\n",
    "                       use_recursion,\n",
    "                       fill_na,\\\n",
    "                       show_progress, \n",
    "                       early_stopping = None):\n",
    "        '''\n",
    "        Инициализирует модель для отбора признаков\n",
    "        \n",
    "        Параметры:\n",
    "            1) estimator - модель\n",
    "            2) metric - метрика качества (названия метрик sklearn + может быть кастомная)\n",
    "            3) larger_is_better - критерий оптимизации (чем больше, тем лучше)\n",
    "            4) cv - схема валидации\n",
    "            5) use_values - индексы столбцов, в которых требуется отобрать значения\n",
    "            6) use_recursion - использовать рекурсию в отборе\n",
    "            7) fill_na - значение, которым заполняются np.nan\n",
    "            8) show_progress - печатать результаты валидации\n",
    "            9) early_stopping - число итераций без улучшения метрики для ранней остановки отбора\n",
    "        Возвращает:\n",
    "            1) fit - производит отбор признаков\n",
    "            2) transform - оставляет отобранные признаки\n",
    "            3) return_self - возвращает \n",
    "                - best_features - отобранные признаки(список)\n",
    "                - D_best_features - отобранные значения признаков (словарь: {признак:значения})\n",
    "                - best_score - лучшее значение метрики\n",
    "        '''\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.cv = cv\n",
    "        self.use_values = use_values        \n",
    "        self.use_recursion = use_recursion\n",
    "        self.show_progress = show_progress\n",
    "        self.early_stopping = early_stopping\n",
    "        self.fill_na = fill_na\n",
    "        self.larger_is_better = larger_is_better\n",
    "    def fit(self, X, Y):        \n",
    "        flag = isinstance(X[:, 0], csc_matrix)\n",
    "        # список с результатами валидации\n",
    "        column_value_score = []\n",
    "        # проходим по признакам\n",
    "        for i in tqdm_notebook(range(X.shape[1])):\n",
    "            # если формат матрицы признаков == csc_matrix\n",
    "            if flag:\n",
    "                # выбираем столбец, преобразуем\n",
    "                ser = pd.DataFrame(X[:, i].todense())[0].values.flatten()\n",
    "            # если формат != csc_matrix\n",
    "            else:\n",
    "                # выбираем столбец\n",
    "                ser = X[:, i]        \n",
    "            # если столбец в списке с проверкой значений \n",
    "            if self.use_values is not None:                \n",
    "                if i in self.use_values:\n",
    "                    # уникальные значения столбца\n",
    "                    unique_values = np.unique(ser)  \n",
    "                    # валидируем каждое значение\n",
    "                    for val in unique_values:\n",
    "                        _x = np.int32(ser==val).reshape(-1,1)\n",
    "                        column_value_score.append((i, val,\\\n",
    "                                                   cross_val_score(self.estimator,\\\n",
    "                                                                   _x, Y,\\\n",
    "                                                                   scoring = self.metric,\\\n",
    "                                                                   cv = self.cv).mean()))\n",
    "                else: \n",
    "                    # валидируем столбец\n",
    "                    column_value_score.append((i, None,\\\n",
    "                                               cross_val_score(self.estimator,\\\n",
    "                                                               _x, Y,\\\n",
    "                                                               scoring = self.metric,\\\n",
    "                                                               cv = self.cv).mean()))\n",
    "            else:\n",
    "                # валидируем столбец\n",
    "                    column_value_score.append((i, None,\\\n",
    "                                               cross_val_score(self.estimator,\\\n",
    "                                                               ser.reshape(-1,1), Y,\\\n",
    "                                                               scoring = self.metric,\\\n",
    "                                                               cv = self.cv).mean()))\n",
    "                \n",
    "\n",
    "        # признаки и значения признаков в порядке убывания валидации\n",
    "        order = np.array(sorted(column_value_score, key = lambda x: x[-1], reverse = True))[:, :2]             \n",
    "        # список лучших признаков\n",
    "        best_features = []\n",
    "        # словарь лучших значений признаков\n",
    "        D_best_features = defaultdict(list)\n",
    "        # список с признаками, не давшими прироста\n",
    "        to_drop = []\n",
    "        \n",
    "        # лучшее значение метрики\n",
    "        if self.larger_is_better:\n",
    "            best_score = 0\n",
    "        else:\n",
    "            best_score = np.inf            \n",
    "        counter = 0\n",
    "        # проходим по признакам и значениям признаков в порядке убывания валидации\n",
    "        for feature, feature_value in tqdm_notebook(order):   \n",
    "\n",
    "            # добавляем текущие признаки/значения\n",
    "            if feature_value is None:\n",
    "                best_features.append(feature)               \n",
    "            else:\n",
    "                D_best_features[feature].append(feature_value)\n",
    "\n",
    "            # обновляем матрицы\n",
    "            L = []\n",
    "            for k, v in D_best_features.items():\n",
    "                if isinstance(X[:, k], csc_matrix):\n",
    "                    L.append(pd.DataFrame(X[:, k].tocsc().todense())[0].apply(lambda x: x if x in v else self.fill_na))\n",
    "                else:\n",
    "                    L.append(pd.Series(X[:, k].flatten()).apply(lambda x: x if x in v else self.fill_na))\n",
    "\n",
    "            if flag:\n",
    "                if (len(best_features)>0) & (len(L)>0):\n",
    "                    _X = csc_matrix(hstack([X[:, best_features], csc_matrix(np.column_stack(L)) ]))\n",
    "                elif (len(best_features)==0) & (len(L)>0):\n",
    "                    _X = csc_matrix(np.column_stack(L))\n",
    "                elif (len(best_features)>0) & (len(L)==0):\n",
    "                    _X = csc_matrix(X[:, best_features])                    \n",
    "                        \n",
    "            else:\n",
    "                if (len(best_features)>0) & (len(L)>0):\n",
    "                    _X = np.column_stack([X[:, best_features], np.column_stack(L)])\n",
    "                elif (len(best_features)==0) & (len(L)>0):\n",
    "                    _X = np.column_stack(L)\n",
    "                elif (len(best_features)>0) & (len(L)==0):\n",
    "                    _X = X[:, best_features] \n",
    "            # считаем валидацию    \n",
    "            current_score = cross_val_score(self.estimator, _X, Y, scoring = self.metric, cv = self.cv).mean()\n",
    "            # если метрика улучшилась\n",
    "            if self.larger_is_better:\n",
    "                if current_score>best_score:\n",
    "                    # обновляем лучшую метрику\n",
    "                    best_score = current_score\n",
    "                    counter = 0\n",
    "                    # печатаем \n",
    "                    if self.show_progress:\n",
    "                        print('new best_score = {}'.format(best_score))\n",
    "                # если метрика не улучшилась\n",
    "                else: \n",
    "                    counter+=1\n",
    "                    # удаляем признак/значение\n",
    "                    if feature_value is None:\n",
    "                        best_features = [val for val in best_features if val != feature]\n",
    "                        to_drop.append((feature, None))\n",
    "                    else:\n",
    "                        D_best_features[feature] = [val for val in D_best_features[feature] if val != feature_value]    \n",
    "                        to_drop.append((feature, feature_value))\n",
    "                    if counter == self.early_stopping:\n",
    "                        break\n",
    "            else:\n",
    "                if current_score<best_score:\n",
    "                    # обновляем лучшую метрику\n",
    "                    best_score = current_score\n",
    "                    counter = 0\n",
    "                    # печатаем \n",
    "                    if self.show_progress:\n",
    "                        print('new best_score = {}'.format(best_score))\n",
    "                    # если метрика не улучшилась\n",
    "                else: \n",
    "                    counter+=1\n",
    "                    # удаляем признак/значение\n",
    "                    if feature_value is None:\n",
    "                        best_features = [val for val in best_features if val != feature]\n",
    "                        to_drop.append((feature, None))\n",
    "                    else:\n",
    "                        D_best_features[feature] = [val for val in D_best_features[feature] if val != feature_value]    \n",
    "                        to_drop.append((feature, feature_value))\n",
    "                    if counter == self.early_stopping:\n",
    "                        break\n",
    "\n",
    "        if self.use_recursion:\n",
    "            # запускаем бесконечный цикл\n",
    "            while True:\n",
    "                # списки лучших признаков до и после\n",
    "                to_drop_before = to_drop\n",
    "                to_drop_after = []\n",
    "                # проходим по признакам и значениям признаков в порядке убывания валидации\n",
    "                for feature, feature_value in tqdm_notebook(to_drop_before):   \n",
    "                    # добавляем текущие признаки/значения\n",
    "                    if feature_value is None:\n",
    "                        best_features.append(feature)               \n",
    "                    else:\n",
    "                        D_best_features[feature].append(feature_value)\n",
    "\n",
    "                    # обновляем матрицы\n",
    "                    L = []\n",
    "                    for k, v in D_best_features.items():\n",
    "                        if isinstance(X[:, k], csc_matrix):\n",
    "                            L.append(pd.DataFrame(X[:, k].tocsc().todense())[0].apply(lambda x: x if x in v else self.fill_na))\n",
    "                        else:\n",
    "                            L.append(pd.Series(X[:, k].flatten()).apply(lambda x: x if x in v else self.fill_na))\n",
    "\n",
    "                    if flag:\n",
    "                        if (len(best_features)>0) & (len(L)>0):\n",
    "                            _X = csc_matrix(hstack([X[:, best_features], csc_matrix(np.column_stack(L)) ]))\n",
    "                        elif (len(best_features)==0) & (len(L)>0):\n",
    "                            _X = csc_matrix(np.column_stack(L))\n",
    "                        elif (len(best_features)>0) & (len(L)==0):\n",
    "                            _X = csc_matrix(X[:, best_features])                    \n",
    "\n",
    "                    else:\n",
    "                        if (len(best_features)>0) & (len(L)>0):\n",
    "                            _X = np.column_stack([X[:, best_features], np.column_stack(L)])\n",
    "                        elif (len(best_features)==0) & (len(L)>0):\n",
    "                            _X = np.column_stack(L)\n",
    "                        elif (len(best_features)>0) & (len(L)==0):\n",
    "                            _X = X[:, best_features] \n",
    "\n",
    "                    # считаем валидацию    \n",
    "                    current_score = cross_val_score(self.estimator, _X, Y, scoring = self.metric, cv = self.cv).mean()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if self.larger_is_better:\n",
    "                        if current_score>best_score:\n",
    "                            # обновляем лучшую метрику\n",
    "                            best_score = current_score\n",
    "                            counter = 0\n",
    "                            # печатаем \n",
    "                            if self.show_progress:\n",
    "                                print('new best_score = {}'.format(best_score))\n",
    "                            # если метрика не улучшилась\n",
    "                        else: \n",
    "                            # удаляем признак/значение\n",
    "                            if feature_value is None:\n",
    "                                best_features = [val for val in best_features if val != feature]\n",
    "                                to_drop_after.append((feature, None))\n",
    "                            else:\n",
    "                                D_best_features[feature] = [val for val in D_best_features[feature] if val != feature_value]    \n",
    "                                to_drop_after.append((feature, feature_value))\n",
    "                    else:\n",
    "                        if current_score<best_score:\n",
    "                            # обновляем лучшую метрику\n",
    "                            best_score = current_score\n",
    "                            counter = 0\n",
    "                            # печатаем \n",
    "                            if self.show_progress:\n",
    "                                print('new best_score = {}'.format(best_score))\n",
    "                        else: \n",
    "                            # удаляем признак/значение\n",
    "                            if feature_value is None:\n",
    "                                best_features = [val for val in best_features if val != feature]\n",
    "                                to_drop_after.append((feature, None))\n",
    "                            else:\n",
    "                                D_best_features[feature] = [val for val in D_best_features[feature] if val != feature_value]    \n",
    "                                to_drop_after.append((feature, feature_value))\n",
    "                    \n",
    "\n",
    "                # если списки одинаковые, останавливаем отбор\n",
    "                if len(to_drop_after) == len(to_drop_before):\n",
    "                    break\n",
    "                # если разные - обновляем списки до и после\n",
    "                else:\n",
    "                    to_drop_before = to_drop_after\n",
    "                    to_drop_after = []\n",
    "                    \n",
    "        self.best_features = best_features\n",
    "        self.D_best_features = D_best_features\n",
    "        self.best_score =best_score\n",
    "        self.flag = flag\n",
    "    def transform(self, X):\n",
    "              \n",
    "        if len(self.best_features) !=0:\n",
    "            x1 = X[:, self.best_features]\n",
    "        else:\n",
    "            x1 = None\n",
    "        if len(list(self.D_best_features.keys())) !=0:\n",
    "            L=[]\n",
    "            for k, v in self.D_best_features.items():\n",
    "                if self.flag:\n",
    "                    L.append(pd.DataFrame(X[:, k].tocsc().todense())[0].apply(lambda x: x if x in v else self.fill_na))                    \n",
    "                else:\n",
    "                    L.append(pd.Series(X[:, k].flatten()).apply(lambda x: x if x in v else self.fill_na))\n",
    "            x2 = np.column_stack(L)\n",
    "        else:\n",
    "            x2 = None\n",
    "            \n",
    "        if (x1 is not None) & (x2 is not None):\n",
    "            if self.flag: \n",
    "                _X = csc_matrix(hstack([x1, x2]))\n",
    "            else:\n",
    "                _X = np.column_stack([x1, x2])\n",
    "                \n",
    "        if (x1 is not None) & (x2 is None):\n",
    "            _X = x1\n",
    "        if (x1 is None) & (x2 is not None):\n",
    "            if self.flag:\n",
    "                _X = csc_matrix(x2)\n",
    "            else:\n",
    "                _X = x2\n",
    "        return _X     \n",
    "        \n",
    "    def return_self(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class StackingRegressor():\n",
    "    def __init__(self, models, cv_strat):\n",
    "        '''\n",
    "        models - список с ансамблем моделей\n",
    "        nfolds - число фолдов для ооф предсказаний\n",
    "        seed - генератор случайных чисел\n",
    "        '''        \n",
    "        self.models = models\n",
    "        self.cv_strat = cv_strat        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        1) обучаем модели на валидации\n",
    "        2) сохраняем обученные модели\n",
    "        '''        \n",
    "        try:\n",
    "            _X, _y = np.array(X), np.array(y)\n",
    "        except:\n",
    "            _X, _y = X, y        \n",
    "        estimators = []\n",
    "        for model in tqdm_notebook(self.models):   \n",
    "            try:\n",
    "                for tr_idx, val_idx in self.cv_strat.split(_X):\n",
    "                    model.fit(_X[tr_idx], _y[tr_idx])\n",
    "                    estimators.append(model)\n",
    "            except:\n",
    "                for tr_idx, val_idx in self.cv_strat.split(_X, _y):\n",
    "                    model.fit(_X[tr_idx], _y[tr_idx])\n",
    "                    estimators.append(model)\n",
    "                    \n",
    "        self.fitted_estimators = estimators\n",
    "    def get_metafeatures(self, X):\n",
    "        '''\n",
    "        с помощью обученных моделей получаем метапризнаки\n",
    "        '''\n",
    "        try:\n",
    "            _X = np.array(X)\n",
    "        except:\n",
    "            _X = X\n",
    "        L = []    \n",
    "        for estimator in self.fitted_estimators:\n",
    "            L.append(estimator.predict(_X))\n",
    "        return np.column_stack(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['time%d' % i for i in range(1, 11)]\n",
    "sites = ['site%d' % i for i in range(1, 11)]\n",
    "\n",
    "df_tr = pd.read_csv('train_sessions.csv', parse_dates = times).set_index('session_id').sort_values('time1')\n",
    "df_te = pd.read_csv('test_sessions.csv', parse_dates = times).set_index('session_id')\n",
    "y_tr = df_tr['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 13\n",
    "FILL_NA = -999\n",
    "LEN = len(df_tr)\n",
    "IDX_SPLIT = np.int32(np.around(LEN*.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставляем сайты, которые посещались Элис + присутствуют в тесте\n",
    "sites_to_use = np.intersect1d(pd.Series(df_tr[df_tr['target'] == 1][sites].values.flatten()).dropna().unique(),\\\n",
    "                              pd.Series(df_te[sites].values.flatten()).dropna().unique())\n",
    "\n",
    "# трейн\n",
    "sites_tr = df_tr[sites].applymap(lambda x: x if x in sites_to_use else np.nan)\n",
    "# тест\n",
    "sites_te = df_te[sites]\n",
    "# трейн + тест\n",
    "sites_full = pd.concat([sites_tr, sites_te], 0)\n",
    "# переводим в строку\n",
    "sites_full_str = sites_full.fillna(FILL_NA).astype(str).apply(lambda row:'_'.join(row), axis = 1)\n",
    "\n",
    "# дата начала сессии\n",
    "start_tr = df_tr[times].min(1)\n",
    "start_te = df_te[times].min(1)\n",
    "ts_columns = ['день_года', 'неделя_года', 'год', 'меяц',\\\n",
    "              'день', 'день_недели', 'час', 'минута1','минута2', 'секунда1', 'секунда2']\n",
    "\n",
    "ts_D = {}\n",
    "for name, ser in zip(('tr', 'te'), (start_tr, start_te)):\n",
    "    _df = pd.concat([ser.dt.dayofyear,\\\n",
    "                     ser.dt.weekofyear,\\\n",
    "                     ser.dt.year,\\\n",
    "                     ser.dt.month,\\\n",
    "                     ser.dt.day,\\\n",
    "                     ser.dt.dayofweek,\\\n",
    "                     ser.dt.hour,\\\n",
    "                     ser.dt.minute//60,\\\n",
    "                     ser.dt.minute%60,\\\n",
    "                     ser.dt.second//60,\\\n",
    "                     ser.dt.second%60], 1)\n",
    "    _df.columns = ts_columns    \n",
    "    ts_D[name] = _df\n",
    "\n",
    "# оставляем значения из теста\n",
    "for col in ts_columns:\n",
    "    to_use = ts_D['te'][col].dropna().unique()\n",
    "    ts_D['tr'][col] = ts_D['tr'][col].apply(lambda x: x if x in to_use else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "_X_ts_ohe_tr = csc_matrix(encoder.fit_transform(ts_D['tr'].fillna(FILL_NA)))\n",
    "_X_ts_ohe_te = csc_matrix(encoder.transform(ts_D['te'].fillna(FILL_NA)))\n",
    "\n",
    "# нормализуем \n",
    "mnmx_scaler = MinMaxScaler()\n",
    "mnmx_scaler.fit(ts_D['tr'].fillna(FILL_NA))\n",
    "_X_ts_tr = mnmx_scaler.transform(ts_D['tr'].fillna(FILL_NA))\n",
    "_X_ts_te = mnmx_scaler.transform(ts_D['te'].fillna(FILL_NA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _csc_hstack_arrays(arrays):\n",
    "    ''' объединяет массивы типа csc_matrix (по колонкам)'''\n",
    "    return csc_matrix(hstack(arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_data(sites_full_str, \\\n",
    "                  _X_ts_ohe_tr, _X_ts_ohe_te,\\\n",
    "                  _X_ts_tr, _X_ts_te,\\\n",
    "                  y_tr, vec):\n",
    "    ''' \n",
    "    готовит наборы данных\n",
    "    1) трейн(ТРЕЙН+ОТЛОЖЕННАЯ), тест\n",
    "    2) для логита, для бустинга\n",
    "        \n",
    "    '''\n",
    "    _y_TR, _y_HOLD = np.array(y_tr)[:IDX_SPLIT], np.array(y_tr)[IDX_SPLIT:]\n",
    "    \n",
    "    # tfidf    \n",
    "    tfidf_full = csc_matrix(vec.fit_transform(sites_full_str))\n",
    "    tfidf_tr, tfidf_te = tfidf_full[:LEN], tfidf_full[LEN:]\n",
    "        \n",
    "    _X_logit_tr = _csc_hstack_arrays([_X_ts_ohe_tr, tfidf_tr])\n",
    "    _X_logit_TR, _X_logit_HOLD = _X_logit_tr[:IDX_SPLIT], _X_logit_tr[IDX_SPLIT:]\n",
    "    _X_logit_te = _csc_hstack_arrays([_X_ts_ohe_te, tfidf_te])\n",
    "    \n",
    "    _X_lgb_tr = _csc_hstack_arrays([_X_ts_tr, tfidf_tr])\n",
    "    _X_lgb_TR, _X_lgb_HOLD = _X_lgb_tr[:IDX_SPLIT], _X_lgb_tr[IDX_SPLIT:]\n",
    "    _X_lgb_te = _csc_hstack_arrays([_X_ts_te, tfidf_te])\n",
    "        \n",
    "       \n",
    "    values = (_X_logit_tr, _X_logit_TR, _X_logit_HOLD, _X_logit_te,\\\n",
    "              _X_lgb_tr, _X_lgb_TR, _X_lgb_HOLD, _X_lgb_te,_y_TR, _y_HOLD)\n",
    "    names = ['logit_tr', 'logit_TR', 'logit_HOLD', 'logit_te',\\\n",
    "             'lgb_tr', 'lgb_TR', 'lgb_HOLD', 'lgb_te', 'y_TR', 'y_HOLD']\n",
    "    \n",
    "    return dict(zip(names, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"отбор_0402.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модели\n",
    "logit_clf = LogisticRegression(random_state = SEED)\n",
    "lgb_clf = LGBMClassifier(random_state = SEED)\n",
    "\n",
    "L_N_FOLDS = [3, 4, 5]\n",
    "L_NGRAM_RANGE_MAX = np.arange(1, 11)\n",
    "L_MAXFEATIRES = [100, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12674799703f4d30be46839a26d674af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b3550bae324a01980cf266b6ed6bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83037731b0194c8fb76e1094f7f5c5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b49ff3639034dc68799a52c894afcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=283.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3212a4817f2246369dc9efd84d0403a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=283.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8126a1f087dc450daa80e0ec9e0a42ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=111.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e96145cf05d4ca3913aa88e980d7212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=282.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "D ={}\n",
    "\n",
    "for N in tqdm_notebook(L_N_FOLDS):\n",
    "    tscv = TimeSeriesSplit(N)\n",
    "\n",
    "    # инициализируем модели отбора признаков\n",
    "    lgb_selector = FeatureSelector(estimator = lgb_clf,\\\n",
    "                                   metric = 'roc_auc',\\\n",
    "                                   larger_is_better = True,\\\n",
    "                                   cv = tscv,\\\n",
    "                                   use_values = np.arange(_X_ts_tr.shape[1]),\\\n",
    "                                   use_recursion = False,\\\n",
    "                                   fill_na = FILL_NA,\\\n",
    "                                   show_progress = False)\n",
    "    logit_selector = FeatureSelector(estimator = logit_clf,\\\n",
    "                                     metric = 'roc_auc',\\\n",
    "                                     larger_is_better = True,\\\n",
    "                                     cv = tscv,\\\n",
    "                                     use_values = None,\\\n",
    "                                     use_recursion = False,\\\n",
    "                                     fill_na = FILL_NA,\\\n",
    "                                     show_progress = False)\n",
    "    \n",
    "    L2_scores = []\n",
    "    L2_features = []\n",
    "    for _ngram_range_max in tqdm_notebook(L_NGRAM_RANGE_MAX):  \n",
    "        for _max_features in tqdm_notebook(L_MAXFEATIRES):\n",
    "            D_datasets = _prepare_data(sites_full_str,\\\n",
    "                                       _X_ts_ohe_tr, _X_ts_ohe_te,\\\n",
    "                                       _X_ts_tr, _X_ts_te,\\\n",
    "                                       y_tr = y_tr,\\\n",
    "                                       vec = TfidfVectorizer(ngram_range = (1, _ngram_range_max),\\\n",
    "                                                             max_features = _max_features))\n",
    "            # отбираем признаки\n",
    "            logit_selector.fit(D_datasets['logit_TR'], D_datasets['y_TR'])        \n",
    "            x1_TR = logit_selector.transform(D_datasets['logit_TR'])\n",
    "            x1_HOLD = logit_selector.transform(D_datasets['logit_HOLD'])\n",
    "            x1_tr = logit_selector.transform(D_datasets['logit_tr'])\n",
    "            x1_te = logit_selector.transform(D_datasets['logit_te'])\n",
    "\n",
    "            # тестируем на отложенной\n",
    "            logit_clf.fit(x1_TR, D_datasets['y_TR'])\n",
    "            auc_logit_hold = roc_auc_score(D_datasets['y_HOLD'], logit_clf.predict_proba(x1_HOLD)[:, 1])\n",
    "\n",
    "            # отбираем признаки\n",
    "            lgb_selector.fit(D_datasets['lgb_TR'], D_datasets['y_TR'])        \n",
    "            x2_TR = lgb_selector.transform(D_datasets['lgb_TR'])\n",
    "            x2_HOLD = lgb_selector.transform(D_datasets['lgb_HOLD'])\n",
    "            x2_tr = lgb_selector.transform(D_datasets['lgb_tr'])\n",
    "            x2_te = lgb_selector.transform(D_datasets['lgb_te'])\n",
    "\n",
    "            # тестируем на отложенной\n",
    "            lgb_clf.fit(x2_TR, D_datasets['y_TR'])\n",
    "            auc_lgb_hold = roc_auc_score(D_datasets['y_HOLD'], lgb_clf.predict_proba(x2_HOLD)[:, 1])        \n",
    "\n",
    "            # собираем метрики, признаки\n",
    "            L2_scores.append((logit_selector.best_score, auc_logit_hold, lgb_selector.best_score, auc_lgb_hold))\n",
    "            L2_features.append(((x1_TR, x1_HOLD, x1_tr, x1_te),(x2_TR, x2_HOLD, x2_tr, x2_te)))    \n",
    "    D[N] = (L2_scores, L2_features)\n",
    "    \n",
    "end = time.time()\n",
    "end = time.time()\n",
    "duration = (end - start) // 60\n",
    "print('выполнено за {} часов, {} минут'.format(duration//60, duration%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"стекинг_0402.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklern.svm import LinearSVR¶\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# базовые модели\n",
    "BASE_MODELS_LINEAR = [Lasso(random_state = SEED),\\\n",
    "                      Ridge(random_state = SEED),\n",
    "                      KNeighborsRegressor(),\\\n",
    "                      LinearSVR(random_state = SEED)]\n",
    "BASE_MODELS_TREE = [RandomForestRegressor(random_state = SEED),\\\n",
    "                    LGBMRegressor(random_state = SEED),\\\n",
    "                    XGBRegressor(random_state = SEED),\\               \n",
    "                    KNeighborsRegressor(),\\\n",
    "                    DecisionTreeRegressor(random_state = SEED)]\n",
    "# итераций стекинга\n",
    "N_ITERATIONS = 5\n",
    "# число фолдов\n",
    "N_FOLDS = 20\n",
    "# доли признаков, на которых обучаются базовые модели\n",
    "FEAT_USE_SHARE = .85\n",
    "# итераций обучения на подпространствах признаков\n",
    "N_SUBSET_ITERATIONS = 10\n",
    "\n",
    "# признаки\n",
    "L_logit_TR, L_logit_HOLD, L_logit_tr, L_logit_te = [], [], [], []\n",
    "L_lgb_TR, L_lgb_HOLD, L_lgb_tr, L_lgb_te = [], [], [], []\n",
    "for element in L2_features:\n",
    "    L_logit_TR.append(element[0][0])\n",
    "    L_logit_HOLD.append(element[0][1])\n",
    "    L_logit_tr.append(element[0][2])\n",
    "    L_logit_te.append(element[0][3])\n",
    "    \n",
    "    L_lgb_TR.append(element[1][0])\n",
    "    L_lgb_HOLD.append(element[1][1])\n",
    "    L_lgb_tr.append(element[1][2])\n",
    "    L_lgb_te.append(element[1][3])\n",
    "    \n",
    "logits_L = (L_logit_TR, L_logit_HOLD, L_logit_tr, L_logit_te)\n",
    "lgbs_L = (L_lgb_TR, L_lgb_HOLD, L_lgb_tr, L_lgb_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_logits_meta_TR, L_logits_meta_HOLD, L_logits_meta_tr, L_logits_meta_te = [], [], [], []\n",
    "# запускаем стекинг N_ITERATIONS раз\n",
    "for _ in tqdm_notebook(range(N_ITERATIONS)): \n",
    "    \n",
    "    # в качестве базовых моделей берем линейные\n",
    "    stacking_reg = StackingRegressor(models = BASE_MODELS_LINEAR,\\\n",
    "                                     cv_strat = KFold(N_FOLDS, random_state = _))\n",
    "    \n",
    "    for feat_idx, features in tqdm_notebook(enumerate(logits_L)):\n",
    "        _TR, _HOLD, _tr, _te = features\n",
    "        _y_TR, _y_HOLD = np.array(y_tr)[:IDX_SPLIT], np.array(y_tr)[IDX_SPLIT:]\n",
    "        # обучаемся на разных подпространствах признаков\n",
    "        for subset_idx in tqdm_notebook(range(1, N_SUBSET_ITERATIONS+1)):\n",
    "            \n",
    "            # фиксируем сид\n",
    "            np.random.seed(SEED+feat_idx+1+subset_idx)\n",
    "                \n",
    "            # выбираем признаки, которые будем использовать\n",
    "            nfeatures = _TR.shape[1]\n",
    "            n_feat_use = np.int32(np.around(nfeatures*FEAT_USE_SHARE))\n",
    "            feat_subset = np.random.choice(np.arange(nfeatures), n_feat_use)\n",
    "                \n",
    "            # фитим ансамбль\n",
    "            stacking_reg.fit(_TR[:, feat_subset], _y_TR)\n",
    "\n",
    "            # получаем метапризнаки\n",
    "            L_logits_meta_TR.append(stacking_reg.get_metafeatures(_TR[:, feat_subset]))\n",
    "            L_logits_meta_HOLD.append(stacking_reg.get_metafeatures(_HOLD[:, feat_subset]))\n",
    "            L_logits_meta_tr.append(stacking_reg.get_metafeatures(_tr[:, feat_subset]))\n",
    "            L_logits_meta_te.append(stacking_reg.get_metafeatures(_te[:, feat_subset]))\n",
    "            \n",
    "L_lgbs_meta_TR, L_lgbs_meta_HOLD, L_lgbs_meta_tr, L_lgbs_meta_te = [], [], [], []\n",
    "# запускаем стекинг N_ITERATIONS раз\n",
    "for _ in tqdm_notebook(range(N_ITERATIONS)): \n",
    "    \n",
    "    # в качестве базовых моделей берем линейные\n",
    "    stacking_reg = StackingRegressor(models = BASE_MODELS_TREE,\\\n",
    "                                     cv_strat = KFold(N_FOLDS, random_state = _))\n",
    "    \n",
    "    for feat_idx, features in tqdm_notebook(enumerate(lgbs_L)):\n",
    "        _TR, _HOLD, _tr, _te = features\n",
    "        _y_TR, _y_HOLD = np.array(y_tr)[:IDX_SPLIT], np.array(y_tr)[IDX_SPLIT:]\n",
    "        # обучаемся на разных подпространствах признаков\n",
    "        for subset_idx in tqdm_notebook(range(1, N_SUBSET_ITERATIONS+1)):\n",
    "            \n",
    "            # фиксируем сид\n",
    "            np.random.seed(SEED+feat_idx+1+subset_idx)\n",
    "                \n",
    "            # выбираем признаки, которые будем использовать\n",
    "            nfeatures = _TR.shape[1]\n",
    "            n_feat_use = np.int32(np.around(nfeatures*FEAT_USE_SHARE))\n",
    "            feat_subset = np.random.choice(np.arange(nfeatures), n_feat_use)\n",
    "                \n",
    "            # фитим ансамбль\n",
    "            stacking_reg.fit(_TR[:, feat_subset], _y_TR)\n",
    "\n",
    "            # получаем метапризнаки\n",
    "            L_lgbs_meta_TR.append(stacking_reg.get_metafeatures(_TR[:, feat_subset]))\n",
    "            L_lgbs_meta_HOLD.append(stacking_reg.get_metafeatures(_HOLD[:, feat_subset]))\n",
    "            L_lgbs_meta_tr.append(stacking_reg.get_metafeatures(_tr[:, feat_subset]))\n",
    "            L_lgbs_meta_te.append(stacking_reg.get_metafeatures(_te[:, feat_subset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
