{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABPCAYAAABI4RaTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABvklEQVR4nO3YIW6VURRG0f9BDYImJFSQptApdDoMggCmEoOvxGOZCh7dvCYVhRmQXCbQPgX7XmAte8yntji7McYGQOPR7AEA/xPRBQiJLkBIdAFCogsQEl2A0NGh4/NnT8b56XG1ZWn77Wz2hGWc3exnT1jG7c8Xsycs4/HLH7MnLGP/7eb7GOPkvtvB6J6fHm9fv7z+M6v+Mu+2q9kTlnF1+Wb2hGV8vPswe8Iynn76PHvCMt5evL9+6Oa9ABASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQGg3xnj4uNvdbdt23c0B+Ce8GmOc3Hc4GF0Afi/vBYCQ6AKERBcgJLoAIdEFCP0CN7smTTJjnpgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x86.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, tqdm, json, pickle, gc, zipfile, itertools, time, collections, sys, requests, schedule\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import catboost as cb\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error\n",
    "import catboost as cb\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "custom_colors = [\"#ffd670\",\"#70d6ff\",\"#ff4d6d\",\"#8338ec\",\"#90cf8e\"]\n",
    "customPalette = sns.set_palette(sns.color_palette(custom_colors))\n",
    "sns.palplot(sns.color_palette(custom_colors),size=1.2)\n",
    "plt.tick_params(axis='both', labelsize=0, length = 0)\n",
    "\n",
    "\n",
    "def amex_metric_official(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "\n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def get_metrics(model, X_eval, y_eval):\n",
    "\n",
    "    \"\"\" calulate metrics \"\"\"\n",
    "\n",
    "    pred = model.predict(X_eval)\n",
    "    pred_prob = model.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "    d= {}\n",
    "\n",
    "    d['accuracy'] = accuracy_score(y_eval, pred)\n",
    "    d['f1'] = f1_score(y_eval, pred)\n",
    "    d['auc'] = roc_auc_score(y_eval, pred_prob)\n",
    "\n",
    "    y_true, y_predprob = y_eval.to_frame('target'), pd.Series(pred_prob, index = y_eval.index).to_frame('prediction')\n",
    "    d['amex_metric_official'] = amex_metric_official(y_true, y_predprob)\n",
    "\n",
    "    d['tp'] = ((y_eval==1)&(pred==1)).sum()\n",
    "    d['tn'] = ((y_eval==0)&(pred==0)).sum()\n",
    "    d['fp'] = ((y_eval==0)&(pred==1)).sum()\n",
    "    d['fn'] = ((y_eval==1)&(pred==0)).sum()\n",
    "\n",
    "    d['importances'] = ser_imp = pd.Series(dict(zip(X_eval.columns, model.feature_importances_))).sort_values(ascending = False)\n",
    "\n",
    "    return d\n",
    "\n",
    "def eval_catboost(X_train, y_train, X_eval, y_eval, verbose):\n",
    "\n",
    "    \"\"\" evaluate model \"\"\"\n",
    "\n",
    "    cat_features = np.where(X_train.dtypes=='category')[0]\n",
    "\n",
    "    params_c = {}\n",
    "    params_c['iterations'] = 100\n",
    "    params_c['cat_features'] = cat_features\n",
    "    params_c['od_type'] = 'Iter'\n",
    "    params_c['od_wait'] = 20  \n",
    "    params_c['eval_metric'] = 'AUC' \n",
    "    params_c['verbose'] = verbose\n",
    "\n",
    "    model = cb.CatBoostClassifier(**params_c)\n",
    "    model.fit(X_train, y_train, eval_set = (X_eval, y_eval))    \n",
    "    best_iter = model.best_iteration_\n",
    "    return model, best_iter, get_metrics(model, X_eval, y_eval) \n",
    "\n",
    "def get_catboost_eval_results(X_train, y_train, X_eval, y_eval):\n",
    "\n",
    "    \"\"\" feature selection and eval results \"\"\"\n",
    "\n",
    "    i=1\n",
    "    while True:\n",
    "\n",
    "        print('> iter#{}. n_features: {}'.format(i, X_train.shape[1]))\n",
    "\n",
    "        model, best_iter, d_eval_results = eval_catboost(X_train, y_train, X_eval, y_eval,verbose=1)\n",
    "\n",
    "        mask = model.feature_importances_>0\n",
    "\n",
    "        if np.all(mask):\n",
    "            break\n",
    "        else:\n",
    "            best_features = X_train.columns[mask]\n",
    "            X_train, X_eval = X_train[best_features], X_eval[best_features]\n",
    "            i+=1\n",
    "    i = 1       \n",
    "    while True:\n",
    "\n",
    "        L_perm_imp = []\n",
    "        for _ in tqdm.tqdm(range(10)):\n",
    "            d_perm_imp = permutation_importance(model, X_eval, y_eval, scoring='roc_auc', n_jobs=-1, n_repeats = 1, random_state = _)    \n",
    "            L_perm_imp.append(d_perm_imp['importances_mean'])\n",
    "        mean_perm_imp = np.c_[L_perm_imp].mean(0)\n",
    "\n",
    "        mask = mean_perm_imp>0\n",
    "\n",
    "        if np.all(mask):\n",
    "            break\n",
    "        else:\n",
    "            best_features = X_train.columns[mask]\n",
    "            X_train, X_eval = X_train[best_features], X_eval[best_features]\n",
    "            model, best_iter, d_eval_results = eval_catboost(X_train, y_train, X_eval, y_eval,verbose=0)\n",
    "            i+=1\n",
    "\n",
    "    return {\n",
    "        'iterations':best_iter,        \n",
    "        'eval_results':d_eval_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir with data\n",
    "PATH_TO_DATA= 'data'\n",
    "\n",
    "# seed\n",
    "SEED = 13\n",
    "\n",
    "# target key\n",
    "TARGET_KEY='target'\n",
    "\n",
    "# customer id key\n",
    "ID_KEY = 'customer_ID'\n",
    "\n",
    "DATE_KEY = 'S_2'\n",
    "\n",
    "# split\n",
    "TEST_SIZE = .1\n",
    "\n",
    "# percentiles (10)\n",
    "PS = np.linspace(2.5, 97.5, 10)\n",
    "\n",
    "# batch size\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "DEFAULT_VALUE = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "df_train = pd.read_feather(os.path.join(PATH_TO_DATA, 'train_data.ftr'))\n",
    "\n",
    "# drop target, order by date\n",
    "df_train = df_train.drop(TARGET_KEY, 1).sort_values(DATE_KEY).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique customers\n",
    "unique_customer_id = df_train[ID_KEY].unique()\n",
    "\n",
    "# split\n",
    "id_train, id_eval = train_test_split(unique_customer_id, test_size = TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number\n",
    "num_features = df_train.select_dtypes('number').columns\n",
    "\n",
    "# category \n",
    "cat_features = df_train.select_dtypes('category').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date features\n",
    "df_train['timestamp'] = df_train[DATE_KEY].apply(lambda row: row.timestamp()) / 1e9\n",
    "date_features = np.array(['timestamp'])\n",
    "\n",
    "# round timestamp\n",
    "for i in [1, 2, 3, 4, 5]:\n",
    "    new_key = f'timestamp__round_{i}'\n",
    "    df_train[new_key] = df_train['timestamp'].round(i)\n",
    "    date_features = np.append(date_features, new_key)\n",
    "\n",
    "# year\n",
    "df_train['year'] = df_train[DATE_KEY].dt.year\n",
    "date_features = np.append(date_features, 'year')\n",
    "\n",
    "# month\n",
    "df_train['month'] = df_train[DATE_KEY].dt.month\n",
    "date_features = np.append(date_features, 'month')\n",
    "\n",
    "# day\n",
    "df_train['day'] = df_train[DATE_KEY].dt.day\n",
    "date_features = np.append(date_features, 'day')\n",
    "\n",
    "# weekday\n",
    "df_train['weekday'] = df_train[DATE_KEY].dt.weekday\n",
    "date_features = np.append(date_features, 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['timestamp', 'timestamp__round_1', 'timestamp__round_2',\n",
       "       'timestamp__round_3', 'timestamp__round_4', 'timestamp__round_5',\n",
       "       'year', 'month', 'day', 'weekday'], dtype='<U18')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split customers into buckets\n",
    "unique_id_key = df_train[ID_KEY].value_counts().index\n",
    "l_batches = np.array_split(unique_id_key, np.int32(np.ceil(len(unique_id_key) / BATCH_SIZE)))\n",
    "\n",
    "# maximum amount of records for each customer\n",
    "max_group_size = df_train[ID_KEY].value_counts().max()\n",
    "num_date_features = np.append(num_features, date_features)\n",
    "\n",
    "d_feature_name = {}\n",
    "for key in num_date_features:    \n",
    "    d_feature_name[key] = [f'{key}__ts_{i+1}' for i in range(max_group_size)]\n",
    "\n",
    "def add_rows(subdf, max_group_size, num_features):\n",
    "    ''' pad data for customer to max records size'''\n",
    "    size = subdf.shape[0]\n",
    "    df_add = pd.DataFrame(np.full((max_group_size-size, len(num_features)), np.nan), columns = num_features)    \n",
    "    return df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4be07a5e644d04b69fe97ac0c29439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/459 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd35a28a6ce489e84cfd04e5772d146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_features_train = pd.DataFrame()\n",
    "\n",
    "# for each batch\n",
    "for batch in tqdm.tqdm_notebook(l_batches):\n",
    "\n",
    "    # subset\n",
    "    df = df_train[df_train[ID_KEY].isin(batch)]\n",
    "\n",
    "    # add features\n",
    "    rows = []\n",
    "    for id, subdf in tqdm.tqdm_notebook(df.groupby(ID_KEY)):\n",
    "\n",
    "        # pad customer\n",
    "        subdf_padded = pd.concat([add_rows(subdf, max_group_size, num_date_features), subdf[num_date_features]])\n",
    "\n",
    "        # features\n",
    "        d = {ID_KEY:id}\n",
    "        d.update(subdf[cat_features].iloc[0].to_dict())\n",
    "\n",
    "        # number\n",
    "        for key in num_date_features:\n",
    "\n",
    "            feature_values = subdf_padded[key].values\n",
    "            feature_names = d_feature_name[key]\n",
    "\n",
    "            d.update(dict(zip(feature_names, feature_values)))\n",
    "            del feature_names, feature_values            \n",
    "\n",
    "        s = pd.Series(d)\n",
    "        # agg number\n",
    "        for key in num_date_features:\n",
    "\n",
    "            try:\n",
    "\n",
    "                feature_names = d_feature_name[key]\n",
    "                feature_values = s[feature_names]\n",
    "\n",
    "                d[f'{key}__mean'] = feature_values.mean()\n",
    "                d[f'{key}__sum'] = feature_values.sum()\n",
    "                d[f'{key}__min'] = feature_values.min()\n",
    "                d[f'{key}__max'] = feature_values.max()\n",
    "                d[f'{key}__nan_mean'] = feature_values.isna().mean()\n",
    "                d[f'{key}__diff_mean'] = feature_values.fillna(0).diff().mean()\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        rows.append(d)\n",
    "        del d, s, subdf_padded, subdf\n",
    "\n",
    "    # features subset\n",
    "    subdf_features = pd.DataFrame.from_records(rows)\n",
    "    del rows\n",
    "\n",
    "    # add\n",
    "    df_features_train = df_features_train.append(subdf_features)\n",
    "    del subdf_features \n",
    "\n",
    "    break  \n",
    "\n",
    "# add binning\n",
    "for key in tqdm.tqdm(d_feature_name.keys()):\n",
    "    try:\n",
    "        feature_names = d_feature_name[key]\n",
    "        arr = pd.Series(df_features_train[feature_names].values.flatten()).dropna().values\n",
    "        bins = np.unique(np.percentile(arr, PS))\n",
    "        df_features_train = pd.concat([df_features_train, df_features_train[feature_names].applymap(lambda x: np.abs(bins-x).argmin()).add_suffix('__bin')], 1)\n",
    "        del arr\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_features_train = df_features_train.sort_values(ID_KEY).set_index(ID_KEY)\n",
    "\n",
    "# cat\n",
    "df_features_train[cat_features] = df_features_train[cat_features].astype('object').fillna('default').astype('str').astype('category')\n",
    "\n",
    "# num binned\n",
    "bin_features = df_features_train.columns[df_features_train.columns.str.contains('__bin')]\n",
    "df_features_train[bin_features] = df_features_train[bin_features].astype('str').astype('category')\n",
    "\n",
    "# num\n",
    "num_features = df_features_train.select_dtypes('number').columns\n",
    "df_features_train[num_features] = df_features_train[num_features].fillna(DEFAULT_VALUE)\n",
    "\n",
    "# scale num\n",
    "scaler = MinMaxScaler()\n",
    "for key in tqdm.tqdm(num_features):\n",
    "    df_features_train[key] = scaler.fit_transform(df_features_train[[key]]).flatten()\n",
    "\n",
    "assert df_features_train.isna().any().any()==False\n",
    "\n",
    "df_features_train.to_pickle('df_features_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "899f0a3c5ad0e42f7243cad936495903bada6a93186c371d9f61c48e98383a75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
