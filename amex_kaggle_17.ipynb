{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import stats\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import gc, os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "\n",
    "    cat_keys = [\n",
    "        \"B_30\",\"B_38\",\"D_114\", \"D_116\", \"D_117\", \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"\n",
    "    ]\n",
    "\n",
    "    num_keys = [\n",
    "        'P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42', 'D_43', 'D_44', 'B_4', 'D_45', 'B_5',\n",
    "        'R_2', 'D_46', 'D_47', 'D_48', 'D_49', 'B_6', 'B_7', 'B_8', 'D_50', 'D_51', 'B_9', 'R_3', 'D_52', 'P_3',\n",
    "        'B_10', 'D_53', 'S_5', 'B_11', 'S_6', 'D_54', 'R_4', 'S_7', 'B_12', 'S_8', 'D_55', 'D_56', 'B_13', 'R_5',\n",
    "        'D_58', 'S_9', 'B_14', 'D_59', 'D_60', 'D_61', 'B_15', 'S_11', 'D_62', 'D_65', 'B_16', 'B_17', 'B_18', 'B_19',\n",
    "        'B_20', 'S_12', 'R_6', 'S_13', 'B_21', 'D_69', 'B_22', 'D_70', 'D_71', 'D_72', 'S_15', 'B_23', 'D_73',\n",
    "        'P_4', 'D_74', 'D_75', 'D_76', 'B_24', 'R_7', 'D_77', 'B_25', 'B_26', 'D_78', 'D_79', 'R_8', 'R_9', 'S_16',\n",
    "        'D_80', 'R_10', 'R_11', 'B_27', 'D_81', 'D_82', 'S_17', 'R_12', 'B_28', 'R_13', 'D_83', 'R_14', 'R_15', 'D_84',\n",
    "        'R_16', 'B_29', 'S_18', 'D_86', 'D_87', 'R_17', 'R_18', 'D_88', 'B_31', 'S_19', 'R_19', 'B_32', 'S_20', 'R_20', 'R_21',\n",
    "        'B_33', 'D_89', 'R_22', 'R_23', 'D_91', 'D_92', 'D_93', 'D_94', 'R_24', 'R_25', 'D_96', 'S_22', 'S_23', 'S_24', 'S_25',\n",
    "        'S_26', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'B_36', 'B_37', 'R_26', 'R_27', 'D_108', 'D_109', 'D_110', 'D_111',\n",
    "        'B_39', 'D_112', 'B_40', 'S_27', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127',\n",
    "        'D_128', 'D_129', 'B_41', 'B_42', 'D_130', 'D_131', 'D_132', 'D_133', 'R_28', 'D_134', 'D_135', 'D_136', 'D_137',\n",
    "        'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145'\n",
    "    ]\n",
    "\n",
    "    id_key = 'customer_ID'\n",
    "    date_key = 'S_2'\n",
    "    max_group_size=13     \n",
    "\n",
    "    def amex_metric(y_true, y_pred):\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "        weights = np.where(labels[:,0]==0, 20, 1)\n",
    "        cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "        top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "        gini = [0,0]\n",
    "        for i in [1,0]:\n",
    "            labels = np.transpose(np.array([y_true, y_pred]))\n",
    "            labels = labels[labels[:, i].argsort()[::-1]]\n",
    "            weight = np.where(labels[:,0]==0, 20, 1)\n",
    "            weight_random = np.cumsum(weight / np.sum(weight))\n",
    "            total_pos = np.sum(labels[:, 0] *  weight)\n",
    "            cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "            lorentz = cum_pos_found / total_pos\n",
    "            gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "    def lgb_amex_metric(y_pred, y_true):\n",
    "\n",
    "        def amex_metric(y_true, y_pred):\n",
    "            labels = np.transpose(np.array([y_true, y_pred]))\n",
    "            labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "            weights = np.where(labels[:,0]==0, 20, 1)\n",
    "            cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "            top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "            gini = [0,0]\n",
    "            for i in [1,0]:\n",
    "                labels = np.transpose(np.array([y_true, y_pred]))\n",
    "                labels = labels[labels[:, i].argsort()[::-1]]\n",
    "                weight = np.where(labels[:,0]==0, 20, 1)\n",
    "                weight_random = np.cumsum(weight / np.sum(weight))\n",
    "                total_pos = np.sum(labels[:, 0] *  weight)\n",
    "                cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "                lorentz = cum_pos_found / total_pos\n",
    "                gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "            return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "        y_true = y_true.get_label()\n",
    "        return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'None', #CFG.metric,\n",
    "        #'boosting': 'dart',\n",
    "        'n_jobs':-1,\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,        \n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40,\n",
    "        'first_metric_only':True\n",
    "    }\n",
    "\n",
    "    path2data = 'data'\n",
    "    seed = 42\n",
    "    skf = StratifiedKFold(5, shuffle = True, random_state=seed)\n",
    "    amex_scorer = make_scorer(amex_metric)\n",
    "    target_keuy = 'target'\n",
    "    pi_iter=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cat_features = [\n",
    "#         \"B_30\",\"B_38\",\"D_114\", \"D_116\", \"D_117\", \"D_120\", \"D_126\", \"D_63\", \"D_64\", \"D_66\", \"D_68\"\n",
    "#     ]\n",
    "# all_cat_features = [f'{k}_last' for k in all_cat_features]+[f'{k}_first' for k in all_cat_features]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet(\"train_fe_v3.parquet\").set_index(CFG.id_key).drop(CFG.target_keuy, 1)\n",
    "with open('y.pickle', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (X.index==y.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = X.iloc[:1000]\n",
    "# y = y.loc[X.index]\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_into_groups_v1(X):\n",
    "    fs1 = X.columns[X.columns.str.contains('last')&~X.columns.str.contains('sub')&~X.columns.str.contains('div')].tolist()\n",
    "    fs2 = X.columns[X.columns.str.contains('first')].tolist()\n",
    "    fs3 = X.columns[X.columns.str.contains('mean')].tolist()\n",
    "    fs4 = X.columns[X.columns.str.contains('min')].tolist()\n",
    "    fs5 = X.columns[X.columns.str.contains('max')].tolist()\n",
    "    fs6 = X.columns[X.columns.str.contains('std')].tolist()\n",
    "    fs7 = X.columns[X.columns.str.contains('sub')].tolist()\n",
    "    fs8 = X.columns[X.columns.str.contains('div')].tolist()\n",
    "    fs9 = X.columns[X.columns.str.contains('diff')].tolist()\n",
    "    fs10 = X.drop(fs1+fs2+fs3+fs4+fs5+fs6+fs7+fs8+fs9, 1).columns\n",
    "    return fs1, fs2, fs3, fs4, fs5, fs6, fs7, fs8, fs9, fs10\n",
    "\n",
    "def calc_cv(X, y, num_boost_round):    \n",
    "    scores, importances = [], []\n",
    "    for i, (tr_idx, val_idx) in tqdm_notebook(enumerate(CFG.skf.split(X,y)), total = CFG.skf.n_splits):   \n",
    "        x_train, x_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[tr_idx], y.iloc[val_idx]    \n",
    "        lgb_tr = lgb.Dataset(x_train, y_train, params={'verbose': -1}) \n",
    "        lgb_val = lgb.Dataset(x_val, y_val, params={'verbose': -1})         \n",
    "        eval_result = {}\n",
    "        model = lgb.train(\n",
    "                params = CFG.params,\n",
    "                train_set = lgb_tr,\n",
    "                valid_sets = [lgb_tr, lgb_val],\n",
    "                num_boost_round = num_boost_round,        \n",
    "                early_stopping_rounds = 100,\n",
    "                verbose_eval = 100,\n",
    "                feval = CFG.lgb_amex_metric,\n",
    "                evals_result=eval_result,            \n",
    "        )\n",
    "        scores.append(model.best_score['valid_1']['amex_metric'])\n",
    "        importances.append(model.feature_importance())\n",
    "        del x_train, x_val, y_train, y_val, lgb_tr, lgb_val\n",
    "        gc.collect()\n",
    "    return np.mean(scores), np.r_[importances].mean(0)\n",
    "\n",
    "def calc_feature_rank(importances, feature_names):\n",
    "    ser_imp2 = pd.Series(dict(zip(feature_names, importances))).sort_values().to_frame('imp')\n",
    "    ser_imp2['rank'] = np.arange(1, len(ser_imp2)+1)\n",
    "    return ser_imp2\n",
    "\n",
    "def sel_feat_with_pi_and_cv(X, y, num_boost_round, n_repeat, seed):\n",
    "\n",
    "    def fit_model(x_train, x_val, y_train, y_val, num_boost_round):\n",
    "        print('\\tevaluating ....')\n",
    "        lgb_tr = lgb.Dataset(x_train, y_train, params={'verbose': -1}) \n",
    "        lgb_val = lgb.Dataset(x_val, y_val, params={'verbose': -1})        \n",
    "        eval_result = {}\n",
    "        model = lgb.train(\n",
    "                params = CFG.params,\n",
    "                train_set = lgb_tr,\n",
    "                valid_sets = [lgb_tr, lgb_val],\n",
    "                num_boost_round = num_boost_round,        \n",
    "                early_stopping_rounds = 100,\n",
    "                verbose_eval = 100,\n",
    "                feval = CFG.lgb_amex_metric,\n",
    "                evals_result=eval_result\n",
    "        )\n",
    "        score = model.best_score['valid_1']['amex_metric']\n",
    "        del x_train, x_val, y_train, y_val, lgb_tr, lgb_val\n",
    "        gc.collect()\n",
    "        return model\n",
    "\n",
    "    def calc_pi(model, x_val, y_val, n_repeat, seed):\n",
    "        print('\\tcalculating permutation importances ...')\n",
    "        l_pi = []\n",
    "        for i in tqdm_notebook(range(n_repeat)):\n",
    "            pi_mean = permutation_importance(\n",
    "                model,\n",
    "                x_val, y_val,\n",
    "                n_repeats=1,\n",
    "                scoring =CFG.amex_scorer,\n",
    "                random_state=CFG.seed+i+seed,\n",
    "                n_jobs=-1\n",
    "            ).importances_mean\n",
    "            l_pi.append(pi_mean) \n",
    "            del pi_mean \n",
    "        pi_mean = np.r_[l_pi].mean(0) \n",
    "        return pi_mean   \n",
    "\n",
    "    ###############################################################################################################\n",
    "\n",
    "    all_best_features = []\n",
    "    for i, (tr_idx, val_idx) in tqdm_notebook(enumerate(CFG.skf.split(X,y)), total = CFG.skf.n_splits):    \n",
    "    \n",
    "        x_train, x_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[tr_idx], y.iloc[val_idx]    \n",
    "\n",
    "        model = fit_model(x_train, x_val, y_train, y_val, num_boost_round)\n",
    "        pi_mean = calc_pi(model, x_val, y_val, n_repeat, seed)        \n",
    "        best_features = x_train.columns[pi_mean>0].tolist() \n",
    "        all_best_features.extend(best_features)\n",
    "\n",
    "        del x_train, x_val, y_train, y_val, model, best_features, pi_mean\n",
    "        gc.collect()\n",
    "    \n",
    "    return pd.Series(all_best_features).value_counts()\n",
    "\n",
    "def select_features(X, y, num_boost_round, n_repeat, seed):\n",
    "    feature_A = X.columns\n",
    "    score_A = calc_cv(X, y, num_boost_round=num_boost_round)[0] \n",
    "    feature_rank = sel_feat_with_pi_and_cv(X, y, num_boost_round=num_boost_round, n_repeat=n_repeat, seed=seed)  \n",
    "    score_B = score_A\n",
    "    for r in tqdm_notebook(np.unique(feature_rank.values)):\n",
    "        feat2use = feature_rank[feature_rank>=r].index\n",
    "        score = calc_cv(X[feat2use], y, num_boost_round=num_boost_round)[0] \n",
    "        if score>score_B:\n",
    "            score_B = score\n",
    "            feature_B = feat2use \n",
    "    if score_B>score_A:\n",
    "        best_features = feature_B\n",
    "    else:\n",
    "        best_features = feature_A\n",
    "    return best_features\n",
    "\n",
    "def forward_selection(X, y, split_func, num_boost_round, n_repeat, seed):\n",
    "\n",
    "    fs1, fs2, fs3, fs4, fs5, fs6, fs7, fs8, fs9, fs10 = split_func(X)\n",
    "    print('feature subset size: {}'.format([len(fs) for fs in [fs1, fs2, fs3, fs4, fs5, fs6, fs7, fs8, fs9, fs10]]))\n",
    "\n",
    "    # select features in every subset\n",
    "    all_best_fs = []\n",
    "    for fs in tqdm_notebook([fs1, fs2, fs3, fs4, fs5, fs6, fs7, fs8, fs9, fs10]):\n",
    "        bf = select_features(X[fs], y, num_boost_round=num_boost_round, n_repeat=n_repeat, seed=seed)\n",
    "        all_best_fs.append(bf.tolist())\n",
    "\n",
    "    # score selected subsets\n",
    "    all_best_scores = np.array([calc_cv(X[fs], y, num_boost_round=num_boost_round)[0]  for fs in all_best_fs])\n",
    "    # rank scores\n",
    "    fs_rank = np.argsort(all_best_scores)[::-1]\n",
    "    # forward selection\n",
    "    drop_fs = set()\n",
    "    for i in tqdm_notebook(range(2, len(fs_rank)+1)):\n",
    "        new_fs = np.concatenate([all_best_fs[j] for j in fs_rank[:i]])    \n",
    "        best_fs = select_features(X[new_fs].drop(list(drop_fs), 1), y, num_boost_round=num_boost_round, n_repeat=n_repeat, seed=seed)\n",
    "        drop_fs.update((set(new_fs)-set(best_fs)))\n",
    "\n",
    "    return best_fs\n",
    "\n",
    "def union_selected(X, y, best_fs_100, best_fs_300, split_func, num_boost_round, n_repeat, seed):\n",
    "    fs_union = np.append(best_fs_100, best_fs_300)\n",
    "    best_fs_union = forward_selection(\n",
    "        X[fs_union], y,\n",
    "        split_func=split_features_into_groups_v1,\n",
    "        num_boost_round=num_boost_round,\n",
    "        n_repeat=n_repeat,\n",
    "        seed=seed\n",
    "    )\n",
    "    best_fs_union\n",
    "    return best_fs_union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 1637)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dropped features\n",
    "# drop_fs_100 = X.drop(best_fs_100, 1).columns\n",
    "\n",
    "# # forward selection dropped features with 300 iterations\n",
    "# best_fs_300 = forward_selection(\n",
    "#     X[drop_fs_100], y,\n",
    "#     split_func=split_features_into_groups_v1,\n",
    "#     num_boost_round=300,\n",
    "#     n_repeat=1,\n",
    "#     seed=0\n",
    "# )\n",
    "\n",
    "# # forward selection union of selected features\n",
    "# best_fs_union = union_selected(\n",
    "#     X, y, \n",
    "#     best_fs_100, best_fs_300, \n",
    "#     split_func=split_features_into_groups_v1, \n",
    "#     num_boost_round=300, \n",
    "#     n_repeat=1, \n",
    "#     seed=1\n",
    "# )\n",
    "\n",
    "# # all scores\n",
    "# score_ABC = [calc_cv(X[fs], y, num_boost_round=300)[0]  for fs in [best_fs_100, best_fs_300, best_fs_union]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature subset size: [188, 188, 177, 177, 177, 177, 177, 177, 177, 22]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9deb420de6a423c9e6f79434e489907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dde041bef248c3aebefc799607e0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25594\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765315\tvalid_1's amex_metric: 0.76348\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.765315\tvalid_1's amex_metric: 0.76348\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25611\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766343\tvalid_1's amex_metric: 0.757369\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.766434\tvalid_1's amex_metric: 0.757302\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25613\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.76669\tvalid_1's amex_metric: 0.758689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.766692\tvalid_1's amex_metric: 0.758714\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25602\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.76718\tvalid_1's amex_metric: 0.753646\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.76718\tvalid_1's amex_metric: 0.753646\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25611\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765771\tvalid_1's amex_metric: 0.760683\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.765812\tvalid_1's amex_metric: 0.76105\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9edf72ea84464a97696781dba55410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25594\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765315\tvalid_1's amex_metric: 0.76348\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.765315\tvalid_1's amex_metric: 0.76348\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe80bd884a4481ca0a5391c275408c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25611\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766343\tvalid_1's amex_metric: 0.757369\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.766434\tvalid_1's amex_metric: 0.757302\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377c098d2d96427e92e583eb28b2d91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25613\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.76669\tvalid_1's amex_metric: 0.758689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.766692\tvalid_1's amex_metric: 0.758714\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab1d2bce3dc42e48d710ff9c9e7e9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25602\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.76718\tvalid_1's amex_metric: 0.753646\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.76718\tvalid_1's amex_metric: 0.753646\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95704a0678747a5b39a401cf1ef6102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25611\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765771\tvalid_1's amex_metric: 0.760683\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.765812\tvalid_1's amex_metric: 0.76105\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635c2e0e34164b26b599a66a1042570e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933762ccc11442ea8000469395486f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489acbe23f9649a4bdbb2238f6df946f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25259\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766851\tvalid_1's amex_metric: 0.76578\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.766851\tvalid_1's amex_metric: 0.76578\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25274\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.768153\tvalid_1's amex_metric: 0.75793\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.768153\tvalid_1's amex_metric: 0.75793\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25275\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.76827\tvalid_1's amex_metric: 0.759167\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.76827\tvalid_1's amex_metric: 0.759167\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25264\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.769101\tvalid_1's amex_metric: 0.755533\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.769101\tvalid_1's amex_metric: 0.755533\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25275\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.767239\tvalid_1's amex_metric: 0.76212\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.767239\tvalid_1's amex_metric: 0.76212\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc82d827d71241a38fd372a63c30700d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22851\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765867\tvalid_1's amex_metric: 0.763257\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.765867\tvalid_1's amex_metric: 0.763257\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22860\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766839\tvalid_1's amex_metric: 0.757188\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.76695\tvalid_1's amex_metric: 0.756565\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22860\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766106\tvalid_1's amex_metric: 0.758771\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's amex_metric: 0.766249\tvalid_1's amex_metric: 0.758745\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22854\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.767915\tvalid_1's amex_metric: 0.756108\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.767915\tvalid_1's amex_metric: 0.756108\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22861\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 147\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766314\tvalid_1's amex_metric: 0.761818\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.766314\tvalid_1's amex_metric: 0.761818\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9967851202754a58b3f5394b421aaa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20024\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 117\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.764506\tvalid_1's amex_metric: 0.762883\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.764506\tvalid_1's amex_metric: 0.762883\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20032\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 117\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766265\tvalid_1's amex_metric: 0.757704\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.766265\tvalid_1's amex_metric: 0.757704\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20034\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 117\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.766102\tvalid_1's amex_metric: 0.758634\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.766102\tvalid_1's amex_metric: 0.758634\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20032\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 117\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.767368\tvalid_1's amex_metric: 0.753201\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.767368\tvalid_1's amex_metric: 0.753201\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20034\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 117\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765534\tvalid_1's amex_metric: 0.759966\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.765534\tvalid_1's amex_metric: 0.759966\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cab6f4d378459cb1cabbe8a530d74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12267\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 65\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.762468\tvalid_1's amex_metric: 0.760852\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.762468\tvalid_1's amex_metric: 0.760852\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12272\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 65\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.763693\tvalid_1's amex_metric: 0.755452\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.763693\tvalid_1's amex_metric: 0.755452\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12271\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 65\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.76324\tvalid_1's amex_metric: 0.757951\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.763301\tvalid_1's amex_metric: 0.758294\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12270\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 65\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.764491\tvalid_1's amex_metric: 0.753107\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.764491\tvalid_1's amex_metric: 0.753107\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12277\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 65\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.762742\tvalid_1's amex_metric: 0.758327\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.762742\tvalid_1's amex_metric: 0.758327\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497606162b2f481680a4f04a1bdb434e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7311\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 36\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.763503\tvalid_1's amex_metric: 0.763073\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\ttraining's amex_metric: 0.763937\tvalid_1's amex_metric: 0.764287\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7317\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 36\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765427\tvalid_1's amex_metric: 0.755861\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.765427\tvalid_1's amex_metric: 0.755861\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7314\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 36\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.764228\tvalid_1's amex_metric: 0.760893\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.764422\tvalid_1's amex_metric: 0.76095\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7314\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 36\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.765746\tvalid_1's amex_metric: 0.755404\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.76578\tvalid_1's amex_metric: 0.755385\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7317\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 36\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.764171\tvalid_1's amex_metric: 0.759259\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.764464\tvalid_1's amex_metric: 0.759241\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb67a845dfcb45409477bab8858176bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25372\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.632345\tvalid_1's amex_metric: 0.630636\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.63244\tvalid_1's amex_metric: 0.630725\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25386\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634729\tvalid_1's amex_metric: 0.62451\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's amex_metric: 0.634842\tvalid_1's amex_metric: 0.625064\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25380\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.633394\tvalid_1's amex_metric: 0.623347\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.633758\tvalid_1's amex_metric: 0.62399\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25383\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634283\tvalid_1's amex_metric: 0.626006\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.634525\tvalid_1's amex_metric: 0.62597\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25380\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634935\tvalid_1's amex_metric: 0.620939\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.635235\tvalid_1's amex_metric: 0.621412\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd4207e4f2a44bfa8bdb6091f83d065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25372\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.632345\tvalid_1's amex_metric: 0.630636\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.63244\tvalid_1's amex_metric: 0.630725\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400c56744aec4f23966bf37e1f52bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25386\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634729\tvalid_1's amex_metric: 0.62451\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's amex_metric: 0.634842\tvalid_1's amex_metric: 0.625064\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91348e65a2e94f7b853d301f3b6b28d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25380\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.633394\tvalid_1's amex_metric: 0.623347\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.633758\tvalid_1's amex_metric: 0.62399\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c84fc8b1724af1aa63c5b7a74c8549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25383\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634283\tvalid_1's amex_metric: 0.626006\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.634525\tvalid_1's amex_metric: 0.62597\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563ac11f1d734cdba9e26e0b7c4d41fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25380\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634935\tvalid_1's amex_metric: 0.620939\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's amex_metric: 0.635235\tvalid_1's amex_metric: 0.621412\n",
      "Evaluated only: amex_metric\n",
      "\tcalculating permutation importances ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee150bab8a14d1ca3d19ba74786f226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0996d22b5b904ceb909376a3039cc5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca67c6e0bf34cc9b82e92359baff6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24540\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.633091\tvalid_1's amex_metric: 0.629697\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.633091\tvalid_1's amex_metric: 0.629697\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24553\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634506\tvalid_1's amex_metric: 0.625415\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.634506\tvalid_1's amex_metric: 0.625415\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24549\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634749\tvalid_1's amex_metric: 0.624647\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.634749\tvalid_1's amex_metric: 0.624647\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24551\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634237\tvalid_1's amex_metric: 0.625012\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's amex_metric: 0.634321\tvalid_1's amex_metric: 0.625078\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24549\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.635303\tvalid_1's amex_metric: 0.621476\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.635303\tvalid_1's amex_metric: 0.621476\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb29986a84f34002a7815bf955b73753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24014\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 160\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.628983\tvalid_1's amex_metric: 0.625095\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.628983\tvalid_1's amex_metric: 0.625095\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24027\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 160\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.63027\tvalid_1's amex_metric: 0.620037\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.63027\tvalid_1's amex_metric: 0.620037\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24023\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 160\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.629204\tvalid_1's amex_metric: 0.621932\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.629204\tvalid_1's amex_metric: 0.621932\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24025\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 160\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.63058\tvalid_1's amex_metric: 0.618599\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.63058\tvalid_1's amex_metric: 0.618599\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24023\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 160\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.630335\tvalid_1's amex_metric: 0.617046\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.630335\tvalid_1's amex_metric: 0.617046\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1858922d1e7d4e289841215617f97831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22094\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 140\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.623531\tvalid_1's amex_metric: 0.621375\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.623531\tvalid_1's amex_metric: 0.621375\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22101\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 140\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.624874\tvalid_1's amex_metric: 0.616084\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's amex_metric: 0.624987\tvalid_1's amex_metric: 0.615465\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22098\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 140\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.624783\tvalid_1's amex_metric: 0.615925\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.624783\tvalid_1's amex_metric: 0.615925\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22102\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 140\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.6245\tvalid_1's amex_metric: 0.615021\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's amex_metric: 0.624601\tvalid_1's amex_metric: 0.614844\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22102\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 140\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.626456\tvalid_1's amex_metric: 0.609552\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's amex_metric: 0.626534\tvalid_1's amex_metric: 0.61033\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1a1890d35749a99e788182ea395b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16568\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 96\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.632307\tvalid_1's amex_metric: 0.629496\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.632307\tvalid_1's amex_metric: 0.629496\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16573\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 96\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634002\tvalid_1's amex_metric: 0.622835\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.634002\tvalid_1's amex_metric: 0.622835\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16572\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 96\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.632982\tvalid_1's amex_metric: 0.626497\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.632982\tvalid_1's amex_metric: 0.626497\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16574\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 96\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.632292\tvalid_1's amex_metric: 0.626152\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.632292\tvalid_1's amex_metric: 0.626152\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16577\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 96\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634525\tvalid_1's amex_metric: 0.620603\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.634525\tvalid_1's amex_metric: 0.620603\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b25fac791504908bbe7ce4856a80b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9419\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 44\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.633961\tvalid_1's amex_metric: 0.633331\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.633961\tvalid_1's amex_metric: 0.633331\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9420\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 44\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.63515\tvalid_1's amex_metric: 0.625471\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.63515\tvalid_1's amex_metric: 0.625471\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9421\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 44\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634783\tvalid_1's amex_metric: 0.626001\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.634783\tvalid_1's amex_metric: 0.626001\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9419\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 44\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.634549\tvalid_1's amex_metric: 0.628294\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.634549\tvalid_1's amex_metric: 0.628294\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9419\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 44\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.636355\tvalid_1's amex_metric: 0.623538\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.636355\tvalid_1's amex_metric: 0.623538\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d0442eee704969b8339aa39e190bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35632\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.731132\tvalid_1's amex_metric: 0.727775\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.731132\tvalid_1's amex_metric: 0.727775\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35682\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.733321\tvalid_1's amex_metric: 0.72381\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's amex_metric: 0.733358\tvalid_1's amex_metric: 0.723739\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35644\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.731371\tvalid_1's amex_metric: 0.723142\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.731371\tvalid_1's amex_metric: 0.723142\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35628\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.734202\tvalid_1's amex_metric: 0.716561\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.734202\tvalid_1's amex_metric: 0.716561\n",
      "Evaluated only: amex_metric\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35639\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's amex_metric: 0.732421\tvalid_1's amex_metric: 0.72224\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's amex_metric: 0.732421\tvalid_1's amex_metric: 0.72224\n",
      "Evaluated only: amex_metric\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d57c3b3a53414093d885cf4c461642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tevaluating ....\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 35632\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 177\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    }
   ],
   "source": [
    "# forward selection with 100 iterations and feature subsets\n",
    "best_fs_100 = forward_selection(\n",
    "    X, y,\n",
    "    split_func=split_features_into_groups_v1,\n",
    "    num_boost_round=100,\n",
    "    n_repeat=1,\n",
    "    seed=0\n",
    ")\n",
    "with open('best_fs_100.pickle', 'wb') as f:\n",
    "    pickle.dump(best_fs_100, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7741016590366844, 0.7547454657085837, 0.7907211060440213]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 46, 37)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_fs_100), len(best_fs_300), len(best_fs_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "899f0a3c5ad0e42f7243cad936495903bada6a93186c371d9f61c48e98383a75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
