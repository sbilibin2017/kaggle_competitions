{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import vstack, hstack, csc_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit,\\\n",
    "                                    cross_val_score, cross_validate, GridSearchCV, ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import category_encoders as ce\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "import gc\n",
    "import nltk                                         #Natural language processing tool-kit\n",
    "\n",
    "from nltk.corpus import stopwords                   #Stopwords corpus\n",
    "from nltk.stem import PorterStemmer                 # Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для чтения json файлов\n",
    "def read_json(path_to_file):\n",
    "    with open(path_to_file) as fin:\n",
    "        for line in fin: \n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пути к файлам, размеры файлов\n",
    "PATH_TR, PATH_TE = 'train.json', 'test.json'\n",
    "LEN_TR, LEN_TE = 62313, None\n",
    "\n",
    "y_ser= pd.read_csv('train_log1p_recommends.csv', index_col='id')['log_recommends']\n",
    "y_tr = y_ser.values\n",
    "y_index= y_ser.index\n",
    "\n",
    "IDX_SPLIT = np.int32(LEN_TR*.9)\n",
    "NFOLDS = 3\n",
    "SEED = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_string(string):\n",
    "    '''\n",
    "    1) приводит к нижнему регистру\n",
    "    2) удаляет пробелы в конце строки\n",
    "    3) удаляет эмоджи\n",
    "    4) удаляет стоп-слова\n",
    "    5) делает стемминг(приводит к начальной форме, удаляя окончания и тд)\n",
    "    6) удаляет пунктуацию\n",
    "    '''\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  \n",
    "                u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    string = string.lower().strip() # нижний регистр                \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    string = re.sub(cleanr, ' ', string) # удаляем html теги\n",
    "    string = re.sub(r'[?|!|\\'|\"|#]',r'',string)\n",
    "    string = re.sub(r'[.|,|)|(|\\|/]',r' ',string) # удаляем пунтктуацию\n",
    "    string = emoji_pattern.sub(r'', string).strip() # удаляем эмоджи\n",
    "\n",
    "    words = []\n",
    "    for word in string.split():\n",
    "        # удаляем стоп-слова, делаем стемминг\n",
    "        if word not in stopwords.words('english'):\n",
    "            words.append(snow.stem(word))\n",
    "            \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bdc866169245029fa876a5eb2ebd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=62313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strings_L, string_idxs = [], []\n",
    "for idx, article in tqdm_notebook(zip(y_index, read_json(PATH_TR)), total = LEN_TR):   \n",
    "    string = BeautifulSoup(article['content'], \"lxml\").text     \n",
    "    strings_L.append(_prepare_string(string))\n",
    "    string_idxs.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.array(y_index) == np.array(string_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### целевая переменная "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_boxcox(y,ld):\n",
    "    '''обратное преобразование бокса-кокса'''\n",
    "    if ld == 0:\n",
    "        return(np.exp(y))\n",
    "    else:\n",
    "        return(np.exp(np.log(ld*y+1)/ld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box-cox преобразование(чтобы исправить сильную скошенность)\n",
    "y_boxcox_tr, ld = stats.boxcox(y_tr)\n",
    "\n",
    "plt.hist(y_tr, density = 1, alpha = .7)\n",
    "plt.hist(y_boxcox_tr, density = 1, alpha = .7)\n",
    "plt.legend(['log', 'boxcol+log'])\n",
    "plt.title('target')\n",
    "plt.ylabel('share')\n",
    "plt.xlabel('value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "ridge_reg = Ridge(random_state = SEED)\n",
    "lasso_reg = Lasso(random_state = SEED)\n",
    "lgb_reg = LGBMRegressor(random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_L =[]\n",
    "for i in tqdm_notebook([1, 2, 3]):\n",
    "    vec = TfidfVectorizer(ngram_range = (1, i), max_features = 1000000)\n",
    "    tfidf_tr = csc_matrix(vec.fit_transform(strings_L))\n",
    "    scores = []\n",
    "    for estimator in tqdm_notebook((lin_reg, ridge_reg, lasso_reg, lgb_reg)):\n",
    "        scores.append(cross_validate(estimator, tfidf_tr[:IDX_SPLIT], y_boxcox_tr[:IDX_SPLIT],\n",
    "                       cv=NFOLDS,\n",
    "                       scoring = 'neg_mean_absolute_error')['test_score'].mean())\n",
    "    cv_results_L.append((i, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
